{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdd4bd9-52d1-4123-a18d-385e28d87c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/paul/Desktop/post-crew\n",
      "fatal: destination path 'playpen' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "!pwd\n",
    "!git clone https://github.com/phisad/playpen.git\n",
    "os.chdir(\"playpen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baccd048-df16-4e00-bb7a-57fb6a31ac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE                     key.json\n",
      "README.md                   key.json.template\n",
      "\u001b[1m\u001b[36mclembench\u001b[m\u001b[m                   model_registry.json\n",
      "clembench.log               \u001b[1m\u001b[36mmodels\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mexamples\u001b[m\u001b[m                    \u001b[1m\u001b[36mplaypen\u001b[m\u001b[m\n",
      "game_registry.json          \u001b[1m\u001b[36mplaypen.egg-info\u001b[m\u001b[m\n",
      "game_registry.json.template pyproject.toml\n",
      "fatal: destination path 'clembench' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!pip install -q -e \".[trl]\"\n",
    "#!pip install -q clemcore[huggingface]==2.4.0 playpen auto installs a version between 2.0 > x > 3.0\n",
    "!git clone https://github.com/clp-research/clembench\n",
    "#!pip install -q -r clembench/requirements.txt\n",
    "!echo '[{\"benchmark_path\": \"clembench\"}]' > game_registry.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c21baa73-7c4c-40a0-a53a-cbaec1a4f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_HUB_HTTP_TIMEOUT=60\n",
    "!export HF_HUB_TIMEOUT=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7d991e-b041-4476-9062-c11e8517e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN_R\")\n",
    "keys = {\n",
    "    \"openai\": {\n",
    "        \"organisation\": \"\",\n",
    "        \"api_key\": \"\"\n",
    "    },\n",
    "    \"anthropic\": {\n",
    "        \"api_key\": \"\"\n",
    "    },\n",
    "    \"alephalpha\": {\n",
    "        \"api_key\": \"\"\n",
    "    },\n",
    "    \"huggingface\": {\n",
    "        \"api_key\": f\"{hf_token}\"\n",
    "    },\n",
    "    \"cohere\": {\n",
    "        \"api_key\": \"\"\n",
    "    },\n",
    "    \"generic_openai_compatible\": {\n",
    "        \"api_key\": \"not-needed\",\n",
    "        \"base_url\": \"http://127.0.0.1:8000/v1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "keys_json = json.dumps(keys)\n",
    "with open(\"key.json\", \"w\") as f:\n",
    "    f.write(keys_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea6f2c7-5587-4ed0-8f50-5b917d07633d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ".--------------..--------------..--------------..--------------..--------------..--------------..--------------.\n",
      "|   ______     ||   _____      ||      __      ||  ____  ____  ||   ______     ||  _________   || ____  _____  |\n",
      "|  |_   __ \\   ||  |_   _|     ||     /  \\     || |_  _||_  _| ||  |_   __ \\   || |_   ___  |  |||_   \\|_   _| |\n",
      "|    | |__) |  ||    | |       ||    / /\\ \\    ||   \\ \\  / /   ||    | |__) |  ||   | |_  \\_|  ||  |   \\ | |   |\n",
      "|    |  ___/   ||    | |   _   ||   / ____ \\   ||    \\ \\/ /    ||    |  ___/   ||   |  _|  _   ||  | |\\ \\| |   |\n",
      "|   _| |_      ||   _| |__/ |  || _/ /    \\ \\_ ||    _|  |_    ||   _| |_      ||  _| |___/ |  || _| |_\\   |_  |\n",
      "|  |_____|     ||  |________|  |||____|  |____|||   |______|   ||  |_____|     || |_________|  |||_____|\\____| |\n",
      "'--------------''--------------''--------------''--------------''--------------''--------------''--------------'\n",
      "\n",
      "Found registered model spec that unifies with {\"model_name\":\"smol-135m\"} -> {'model_name': 'smol-135m', 'backend': 'huggingface_local', 'context_size': '2048', 'huggingface_id': 'HuggingFaceTB/SmolLM-135M-Instruct', 'languages': ['en'], 'license': {'name': 'Apache 2.0', 'url': 'https://www.apache.org/licenses/LICENSE-2.0'}, 'lookup_source': '/Users/paul/Desktop/post-crew/playpen/model_registry.json', 'model_config': {'premade_chat_template': True, 'eos_to_cull': '<\\\\|im_end\\\\|>'}, 'open_weight': True, 'parameters': '135M', 'release_date': '2024-09-04'}\n",
      "Found registry entry for backend huggingface_local -> {'backend': 'huggingface_local', 'file_name': 'huggingface_local_api.py', 'file_path': '/Users/paul/miniconda3/envs/clembench/lib/python3.10/site-packages/clemcore/backends/huggingface_local_api.py', 'lookup_source': 'packaged'}\n",
      "Dynamically import backend huggingface_local\n",
      "Successfully loaded smol-135m model\n",
      "  0%|                                                    | 0/63 [00:00<?, ?it/s]/Users/paul/miniconda3/envs/clembench/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "  2%|▋                                        | 1/63 [03:06<3:12:31, 186.31s/it]^C\n"
     ]
    }
   ],
   "source": [
    "!playpen run examples/trl/sft_trainer_simple.py -l smol-135m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f17ed77a-1729-4ac3-ba38-4eb6e72a7c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ".--------------..--------------..--------------..--------------..--------------..--------------..--------------.\n",
      "|   ______     ||   _____      ||      __      ||  ____  ____  ||   ______     ||  _________   || ____  _____  |\n",
      "|  |_   __ \\   ||  |_   _|     ||     /  \\     || |_  _||_  _| ||  |_   __ \\   || |_   ___  |  |||_   \\|_   _| |\n",
      "|    | |__) |  ||    | |       ||    / /\\ \\    ||   \\ \\  / /   ||    | |__) |  ||   | |_  \\_|  ||  |   \\ | |   |\n",
      "|    |  ___/   ||    | |   _   ||   / ____ \\   ||    \\ \\/ /    ||    |  ___/   ||   |  _|  _   ||  | |\\ \\| |   |\n",
      "|   _| |_      ||   _| |__/ |  || _/ /    \\ \\_ ||    _|  |_    ||   _| |_      ||  _| |___/ |  || _| |_\\   |_  |\n",
      "|  |_____|     ||  |________|  |||____|  |____|||   |______|   ||  |_____|     || |_________|  |||_____|\\____| |\n",
      "'--------------''--------------''--------------''--------------''--------------''--------------''--------------'\n",
      "\n",
      "Found registered model spec that unifies with {\"model_name\":\"llama3-8b\"} -> {'model_name': 'llama3-8b', 'backend': 'huggingface_local', 'context_size': '128k', 'huggingface_id': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'languages': ['en', 'de', 'fr', 'it', 'pt', 'hi', 'es', 'th'], 'license': {'name': 'Meta', 'url': 'https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE'}, 'lookup_source': '/home/users/alueser/playpen/model_registry.json', 'model_config': {'requires_api_key': True, 'premade_chat_template': True, 'eos_to_cull': '<\\\\|eot_id\\\\|>'}, 'open_weight': True, 'parameters': '8B', 'release_date': '2024-07-23'}\n",
      "Found registry entry for backend huggingface_local -> {'backend': 'huggingface_local', 'file_name': 'huggingface_local_api.py', 'file_path': '/home/users/alueser/.local/lib/python3.10/site-packages/clemcore/backends/huggingface_local_api.py', 'lookup_source': 'packaged'}\n",
      "Dynamically import backend huggingface_local\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:02<00:00,  1.75it/s]\n",
      "Successfully loaded llama3-8b model\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "  0%|                                                  | 0/2772 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"/home/users/alueser/.local/bin/playpen\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/users/alueser/playpen/playpen/cli.py\", line 104, in main\n",
      "    cli(parser.parse_args())\n",
      "  File \"/home/users/alueser/playpen/playpen/cli.py\", line 79, in cli\n",
      "    train(args.file_path, learner_spec, teacher_spec, args.temperature, args.max_tokens)\n",
      "  File \"/home/users/alueser/playpen/playpen/cli.py\", line 60, in train\n",
      "    playpen_cls(learner_model).learn(game_registry)\n",
      "  File \"/home/users/alueser/playpen/examples/trl/sft_trainer_lora.py\", line 56, in learn\n",
      "    trainer.train()\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 2560, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 3736, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\", line 654, in compute_loss\n",
      "    (loss, outputs) = super().compute_loss(\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/transformers/trainer.py\", line 3801, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/peft/peft_model.py\", line 1757, in forward\n",
      "    return self.base_model(\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 193, in forward\n",
      "    return self.model.forward(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 821, in forward\n",
      "    outputs: BaseModelOutputWithPast = self.model(\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
      "    output = func(self, *args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 571, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 334, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 172, in forward\n",
      "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/users/alueser/.local/lib/python3.10/site-packages/peft/tuners/lora/layer.py\", line 727, in forward\n",
      "    result = result + lora_B(lora_A(dropout(x))) * scaling\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 132.00 MiB. GPU 0 has a total capacty of 79.15 GiB of which 119.75 MiB is free. Process 1174250 has 43.17 GiB memory in use. Process 1214098 has 35.83 GiB memory in use. Of the allocated memory 35.14 GiB is allocated by PyTorch, and 198.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "  0%|                                                  | 0/2772 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!~/.local/bin/playpen run examples/trl/sft_trainer_lora.py -l llama3-8b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "428fe831-3366-4361-861d-d58925084180",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/users/alueser/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e615d66d-dc20-4139-88a8-73c22fd49a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"playpen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ded65a17-33cf-4c82-acde-7e1520a6a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6127315-a0d9-4782-ab0b-c2f5161046ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 24 08:15:53 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:C8:00.0 Off |                    0 |\n",
      "| N/A   58C    P0             303W / 300W |  44237MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d41455-e121-487f-95a4-93eb1016a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!!~/.local/bin/clem run -g \"{'benchmark':['2.0']}\" -m smol-135m-sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1dee74-bc9d-4b01-8790-565aff600979",
   "metadata": {},
   "outputs": [],
   "source": [
    "!!~/.local/bin/clem run -g \"{'benchmark':['2.0']}\" -m llama-8b-sft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clembench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
